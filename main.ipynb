{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:41:50.246694Z",
     "start_time": "2023-11-27T23:41:50.201116Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 12:56:28.149702: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d642a8d311fb98d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:42:01.052020Z",
     "start_time": "2023-11-27T23:41:59.349456Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Mehrdadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Mehrdadi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4562204696664239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:42:03.935552Z",
     "start_time": "2023-11-27T23:42:03.929112Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = open('/Users/Mehrdadi/Downloads/intents.json')\n",
    "data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3072daa01f64e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Identifying Feature and Target for the NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8dab3f9c7f1fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:42:06.665218Z",
     "start_time": "2023-11-27T23:42:06.650672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "data_X = []\n",
    "data_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        data_X.append(pattern)\n",
    "        data_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "words = sorted(set(words))\n",
    "classes =sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13745a210218c935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:42:09.932401Z",
     "start_time": "2023-11-27T23:42:09.917533Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "# creating the bag of words model\n",
    "for idx, doc in enumerate (data_X):\n",
    "    bow = []\n",
    "    text = lemmatizer. lemmatize(doc. lower ())\n",
    "    for word in words:\n",
    "        bow. append(1) if word in text else bow. append (0)\n",
    "\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(data_y[idx])] = 1\n",
    "    # add the one hot encoded Boll and associated classes to training\n",
    "    training. append ([bow, output_row])\n",
    "\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array (training, dtype=object)\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_Y = np. array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc00513433219f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T23:42:10.624247Z",
     "start_time": "2023-11-27T23:42:10.613738Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               7552      \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,848\n",
      "Trainable params: 16,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 1s 3ms/step - loss: 2.8377 - accuracy: 0.0488\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5598 - accuracy: 0.1951\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3982 - accuracy: 0.2927\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3493 - accuracy: 0.3171\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.1191 - accuracy: 0.4390\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8900 - accuracy: 0.4878\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.9674 - accuracy: 0.3415\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.7431 - accuracy: 0.3902\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4057 - accuracy: 0.6341\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3204 - accuracy: 0.5854\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3507 - accuracy: 0.6098\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1561 - accuracy: 0.6341\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2013 - accuracy: 0.6585\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.9876 - accuracy: 0.7317\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0107 - accuracy: 0.7073\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7044 - accuracy: 0.7805\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7932 - accuracy: 0.7073\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6759 - accuracy: 0.7805\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7646 - accuracy: 0.7317\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5268 - accuracy: 0.7805\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5930 - accuracy: 0.7561\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7598 - accuracy: 0.7561\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4479 - accuracy: 0.8537\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5634 - accuracy: 0.7805\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5872 - accuracy: 0.9024\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4951 - accuracy: 0.9024\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3525 - accuracy: 0.9024\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8780\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.8780\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8537\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.9756\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8780\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2162 - accuracy: 0.9268\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9512\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2668 - accuracy: 0.9268\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2638 - accuracy: 0.9024\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.8537\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9268\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2384 - accuracy: 0.9512\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2168 - accuracy: 0.9512\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2423 - accuracy: 0.9268\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2321 - accuracy: 0.9268\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3625 - accuracy: 0.8537\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2037 - accuracy: 0.9268\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9512\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9756\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.9512\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2978 - accuracy: 0.9024\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9512\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2614 - accuracy: 0.9024\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4106 - accuracy: 0.9268\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2452 - accuracy: 0.9268\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9512\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2328 - accuracy: 0.9268\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9756\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2007 - accuracy: 0.8780\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2619 - accuracy: 0.9024\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2359 - accuracy: 0.9024\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1600 - accuracy: 0.9512\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2275 - accuracy: 0.9512\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9268\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2450 - accuracy: 0.8780\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.9268\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3861 - accuracy: 0.9268\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.9756\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9268\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9512\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.9268\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2015 - accuracy: 0.9268\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9512\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3318 - accuracy: 0.8780\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9268\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1434 - accuracy: 0.9512\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1545 - accuracy: 0.9512\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1734 - accuracy: 0.9268\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9756\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9756\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.8537\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9268\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1379 - accuracy: 0.9268\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9756\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9512\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1552 - accuracy: 0.9268\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9268\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.8780\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9756\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9512\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2215 - accuracy: 0.9268\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9512\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1731 - accuracy: 0.9512\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0803 - accuracy: 0.9756\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9756\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9512\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.9512\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1192 - accuracy: 0.9512\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9512\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1356 - accuracy: 0.9512\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9268\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1113 - accuracy: 0.9512\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9512\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1061 - accuracy: 0.9512\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9756\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9512\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9268\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9512\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1593 - accuracy: 0.9024\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9756\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9756\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0805 - accuracy: 0.9512\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9512\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9512\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0543 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9512\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1630 - accuracy: 0.9512\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0842 - accuracy: 0.9512\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1774 - accuracy: 0.9512\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9756\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9512\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9512\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9512\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9756\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9512\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2426 - accuracy: 0.9024\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2061 - accuracy: 0.8780\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0548 - accuracy: 0.9756\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.9756\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1251 - accuracy: 0.9512\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9756\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9756\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1212 - accuracy: 0.9268\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9512\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9512\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9756\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9512\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1133 - accuracy: 0.9268\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9268\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9756\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2383 - accuracy: 0.9268\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2511 - accuracy: 0.9268\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2085 - accuracy: 0.9268\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1103 - accuracy: 0.9512\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0539 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0550 - accuracy: 0.9756\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9756\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139fa1d30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len (train_Y[0]), activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9249ca497f8baa35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text (text):\n",
    "    tokens = nltk. word_tokenize (text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words (text, vocab):\n",
    "    tokens = clean_text (text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens:\n",
    "        for idx, word in enumerate (vocab):\n",
    "            if word == w:\n",
    "                bow[idx] = 1\n",
    "    return np.array (bow)\n",
    "\n",
    "\n",
    "def pred_class (text, vocab, labels):\n",
    "    bow = bag_of_words (text, vocab)\n",
    "    result = model. predict (np.array ([bow]))[0] #extracting probabilities\n",
    "    thresh = 0.5\n",
    "    y_pred = [[indx, res] for indx, res in enumerate (result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True) #Sorting by values of probability in dec\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]]) #Contains labels (tags) for highest probability \n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    if len(intents_list) == 0:\n",
    "        result = \"Sorry! I don't understand.\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"] == tag:\n",
    "                result = random. choice(i[\"responses\"])\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 if you don't want to chat with our ChatBot.\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Hello\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Sorry! I don't understand.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Hello\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "I am feeling good, you?\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "So, everything's okay!\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "That is perfect!\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "That is perfect!\n"
     ]
    }
   ],
   "source": [
    "print(\"Press 0 if you don't want to chat with our ChatBot.\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == 0:\n",
    "        break\n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, data)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
